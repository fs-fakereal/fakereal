{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19461d25-27da-4aa1-9472-c19e46cdca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/liam/Code/school/fakereal/.venv/lib/python3.13/site-packages (25.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73cdd57c-0814-4d76-a1c1-a3e215eaa046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try 'pacman -S\n",
      "\u001b[31m   \u001b[0m python-xyz', where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using 'python -m venv path/to/venv'.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have python-pipx\n",
      "\u001b[31m   \u001b[0m installed via pacman.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cac0fb-de69-4d34-b284-cd5a1be3d905",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m image\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapplications\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VGG16\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input, BatchNormalization\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.train import latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f8b2b5-fc3e-4f22-a853-002243cd3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vgg16\"\n",
    "img_size = 256 # assume same for both width and height\n",
    "batch_size = 8\n",
    "\n",
    "data_dir = \"data\"\n",
    "output_dir = \"output\"\n",
    "# assume there is a json file of the same name inside these data subdirs.\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "checkpoint_path = f\"{checkpoint_dir}/{model_name}/\" + \"cp-{epoch:04d}.weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "01bae064-7dea-460f-9c22-1082f2f806d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train: pd.DataFrame = pd.read_csv(f\"{data_dir}/uniface-ff-train.csv\")\n",
    "df_val: pd.DataFrame = pd.read_csv(f\"{data_dir}/uniface-ff-test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3d25c781-940b-4602-9d59-8e05a7d85d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['label'] = df_train['label'].astype('str')\n",
    "df_val['label'] = df_val['label'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d8760e8e-72a8-4de3-b329-fb3a33cb51d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22993 validated image filenames belonging to 1 classes.\n",
      "Found 4479 validated image filenames belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scout\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 11794 invalid image filename(s) in x_col=\"filepath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "C:\\Users\\scout\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:920: UserWarning: Found 4465 invalid image filename(s) in x_col=\"filepath\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=(1./255),\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[-0.5, 0.5],\n",
    "    rotation_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    df_train,\n",
    "    x_col = \"filepath\",\n",
    "    y_col = \"label\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    validate_filenames=True\n",
    ")\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    df_val,\n",
    "    x_col = \"filepath\",\n",
    "    y_col = \"label\",\n",
    "    target_size=(img_size, img_size),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    validate_filenames=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bbca939-2519-4e6e-b3b5-61e7f5795a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                          </span>┃<span style=\"font-weight: bold\"> Output Shape                  </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Traina… </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ input_layer_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │    <span style=\"font-weight: bold\">-</span>    │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │    <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">N</span>    │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ global_average_pooling2d_3            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │    <span style=\"font-weight: bold\">-</span>    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)              │                               │                │         │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ batch_normalization_2                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │    <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                  │                               │                │         │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │    <span style=\"font-weight: bold\">-</span>    │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                     │            <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │    <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>    │\n",
       "└───────────────────────────────────────┴───────────────────────────────┴────────────────┴─────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTraina…\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n",
       "│ input_layer_7 (\u001b[38;5;33mInputLayer\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │    \u001b[1m-\u001b[0m    │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │     \u001b[38;5;34m14,714,688\u001b[0m │    \u001b[1;91mN\u001b[0m    │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ global_average_pooling2d_3            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │              \u001b[38;5;34m0\u001b[0m │    \u001b[1m-\u001b[0m    │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)              │                               │                │         │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ batch_normalization_2                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │          \u001b[38;5;34m2,048\u001b[0m │    \u001b[1;38;5;34mY\u001b[0m    │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                  │                               │                │         │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                   │              \u001b[38;5;34m0\u001b[0m │    \u001b[1m-\u001b[0m    │\n",
       "├───────────────────────────────────────┼───────────────────────────────┼────────────────┼─────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                     │            \u001b[38;5;34m513\u001b[0m │    \u001b[1;38;5;34mY\u001b[0m    │\n",
       "└───────────────────────────────────────┴───────────────────────────────┴────────────────┴─────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,717,249</span> (56.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,717,249\u001b[0m (56.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,537</span> (6.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,537\u001b[0m (6.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,715,712</span> (56.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,715,712\u001b[0m (56.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(img_size, img_size, 3)\n",
    ")\n",
    "\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "# technique to \"stack\" layers, starting with pretrain model's layers\n",
    "inputs = Input(shape=(img_size, img_size, 3))\n",
    "\n",
    "cl = pretrained_model(inputs, training=False)\n",
    "\n",
    "cl = GlobalAveragePooling2D()(cl)\n",
    "cl = BatchNormalization()(cl)\n",
    "cl = Dropout(0.2)(cl)\n",
    "# cl = Dense(512, activation='relu')(cl)\n",
    "\n",
    "# this is the final layer; size must equal desired output size\n",
    "outputs = Dense(1, activation='sigmoid')(cl)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.summary(show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cd8c4245-b9b5-4f6e-9b14-2d3a6254a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-6\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate),\n",
    "    loss=BinaryCrossentropy(from_logits=False),\n",
    "    metrics=[BinaryAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cc7733d2-f771-4e44-a83d-100f9aecda12",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.path.join(os.getcwd(), checkpoint_path),\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113d2a51-b77d-48e8-b8fd-bc0615302664",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'latest_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m latest = \u001b[43mlatest_checkpoint\u001b[49m(os.path.join(os.getcwd(), checkpoint_path))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(latest)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#model.load_weights(latest)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'latest_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "latest = latest_checkpoint(os.path.join(os.getcwd(), checkpoint_path))\n",
    "print(latest)\n",
    "#model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c573bfc9-c7a5-41f4-95bf-5df02f324140",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scout\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664ms/step - binary_accuracy: 0.5362 - loss: 0.7131   \n",
      "Epoch 1: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0001.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2267s\u001b[0m 789ms/step - binary_accuracy: 0.5362 - loss: 0.7131 - val_binary_accuracy: 0.5693 - val_loss: 0.8077\n",
      "Epoch 2/10\n",
      "\u001b[1m   1/2874\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32:48\u001b[0m 685ms/step - binary_accuracy: 0.5000 - loss: 0.7571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scout\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0002.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 121ms/step - binary_accuracy: 0.5000 - loss: 0.7571 - val_binary_accuracy: 0.5686 - val_loss: 0.8086\n",
      "Epoch 3/10\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 618ms/step - binary_accuracy: 0.6411 - loss: 0.6722   \n",
      "Epoch 3: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0003.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2104s\u001b[0m 732ms/step - binary_accuracy: 0.6412 - loss: 0.6722 - val_binary_accuracy: 0.6713 - val_loss: 0.6240\n",
      "Epoch 4/10\n",
      "\u001b[1m   1/2874\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28:15\u001b[0m 590ms/step - binary_accuracy: 0.6250 - loss: 0.6251\n",
      "Epoch 4: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0004.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 114ms/step - binary_accuracy: 0.6250 - loss: 0.6251 - val_binary_accuracy: 0.6729 - val_loss: 0.6213\n",
      "Epoch 5/10\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 607ms/step - binary_accuracy: 0.7286 - loss: 0.6318   \n",
      "Epoch 5: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0005.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2094s\u001b[0m 729ms/step - binary_accuracy: 0.7286 - loss: 0.6318 - val_binary_accuracy: 0.7200 - val_loss: 0.5400\n",
      "Epoch 6/10\n",
      "\u001b[1m   1/2874\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29:13\u001b[0m 610ms/step - binary_accuracy: 0.8750 - loss: 0.5416\n",
      "Epoch 6: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0006.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m346s\u001b[0m 120ms/step - binary_accuracy: 0.8750 - loss: 0.5416 - val_binary_accuracy: 0.7174 - val_loss: 0.5439\n",
      "Epoch 7/10\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641ms/step - binary_accuracy: 0.8003 - loss: 0.5943   \n",
      "Epoch 7: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0007.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2181s\u001b[0m 759ms/step - binary_accuracy: 0.8003 - loss: 0.5943 - val_binary_accuracy: 0.7446 - val_loss: 0.4997\n",
      "Epoch 8/10\n",
      "\u001b[1m   1/2874\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32:44\u001b[0m 684ms/step - binary_accuracy: 0.6250 - loss: 0.5558\n",
      "Epoch 8: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0008.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 122ms/step - binary_accuracy: 0.6250 - loss: 0.5558 - val_binary_accuracy: 0.7469 - val_loss: 0.4971\n",
      "Epoch 9/10\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 605ms/step - binary_accuracy: 0.8520 - loss: 0.5602   \n",
      "Epoch 9: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0009.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2062s\u001b[0m 717ms/step - binary_accuracy: 0.8520 - loss: 0.5602 - val_binary_accuracy: 0.7775 - val_loss: 0.4506\n",
      "Epoch 10/10\n",
      "\u001b[1m   1/2874\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28:05\u001b[0m 587ms/step - binary_accuracy: 0.8750 - loss: 0.5417\n",
      "Epoch 10: saving model to D:\\scout\\Code\\fakereal\\models\\checkpoints/vgg16/cp-0010.weights.h5\n",
      "\u001b[1m2874/2874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 113ms/step - binary_accuracy: 0.8750 - loss: 0.5417 - val_binary_accuracy: 0.7757 - val_loss: 0.4523\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=(train_generator.samples // train_generator.batch_size),\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=(val_generator.samples // val_generator.batch_size),\n",
    "    callbacks=[checkpoint_callback],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bcb72089-6fea-4933-bd0e-a4061c46072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f\"{output_dir}/deepfake-{model_name}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed20ad08-694c-4d24-a554-4cb886ab6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('data/user/5f88fc6e92fabf4893068c58eb1174d4ed89fadbe3c0448a5854fefd1a01a8f8.jpg', target_size=(img_size, img_size))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "images = np.vstack([x])\n",
    "# classes = model.predict_classes(images, batch_size=2)\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0655ded6-949b-4a17-948d-f13c20a119f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
